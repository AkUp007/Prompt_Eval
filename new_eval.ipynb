{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3692102b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import together\n",
    "\n",
    "load_dotenv()\n",
    "together.api_key = os.getenv(\"TOGETHER_API_KEY\")\n",
    "MODEL_1 = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "MODEL_2 = \"deepseek-ai/DeepSeek-R1-Distill-Qwen-14B\"\n",
    "# MODEL_2 = \"Qwen-QwQ-32B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47fd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate model response\n",
    "def generate_response(prompt, model):\n",
    "    try:\n",
    "        response = together.Complete.create(\n",
    "            prompt=prompt,\n",
    "            model=model,\n",
    "            max_tokens=512,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        return response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in generate_response]: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db40b338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function using LLM as judge\n",
    "def prompteffectiveness(user_prompt, question):\n",
    "    prompteff = f\"\"\"\n",
    "You are an expert evaluator tasked with analyzing a USER PROMPT and a corresponding QUESTION using the following 7 dimensions. Your goal is to critically assess the prompt's quality and effectiveness based on these dimensions, scoring each dimension qualitatively and quantitatively, and then provide a detailed summary.\n",
    "\n",
    "You will evaluate the prompt and question across **7 dimensions**, rating each as:\n",
    "- \"Good\": Strong performance in this aspect\n",
    "- \"Average\": Moderate or partial success\n",
    "- \"Bad\": Weakness or failure\n",
    "- \"N/A\": Not applicable to this prompt/response\n",
    "\n",
    "**Scoring Criteria**: A number between 0 and 10, where:\n",
    "  - **Good**: 8-10 points (8=solid good, 9=very good, 10=excellent)\n",
    "  - **Average**: 4-7 points (4-5=below average, 6-7=above average)\n",
    "  - **Bad**: 0-3 points (0=complete failure, 1-2=poor, 3=weak)\n",
    "  - **N/A**: No score assigned (excluded from overall average calculation)\n",
    "  \n",
    "Then, provide a **Summary** including:\n",
    "**OverallScore: A number between 0 and 10 (rounded to two decimal places), calculated as the average of all dimension scores\n",
    "  - Sum all numerical scores from applicable dimensions\n",
    "  - Divide by number of applicable dimensions (excluding N/A ratings)\n",
    "  - Round to 2 decimal place\n",
    "**ApplicableDimensions: The number of evaluation dimensions applied (typically 7)\n",
    "**PromptEffectiveness: \n",
    "  - Assign based on OverallScore:  \n",
    "    - \"Effective\" if OverallScore ≥ 7.5  \n",
    "    - \"Partially Effective\" if 4 ≤ OverallScore < 7.5  \n",
    "    - \"Ineffective\" if OverallScore < 4\n",
    "**Explanation: \n",
    "  - Concisely justify your ratings, referencing specific dimension scores,  \n",
    "  - Include a brief analysis of prompt strengths and weaknesses,  \n",
    "  - Note any relevant failure or success tags.\n",
    "\n",
    "--- \n",
    "\n",
    "Use the following dimensions to guide your evaluation:\n",
    "### 1. Purpose & Persona\n",
    "**Definition**: \n",
    "- “Purpose” clarifies why the prompt exists (e.g., to summarize, to translate, to analyze).\n",
    "- “Persona” defines for whom or from whose point of view the LLM should answer (e.g., “as a financial analyst,” “as a children’s book author”).\n",
    "**Key Indicators**:\n",
    "  - Does the User Prompt explicitly state its goal?\n",
    "  - Does it assign a clear persona/role to the model?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Both goal and persona are unambiguous.\n",
    "  (Example: “Summarize the following research paper as if you were a science journalist.”)\n",
    "  - Average(4-7): Either goal or persona is stated but one is vague.\n",
    "  (Example: “Write a summary. You’re a journalist.” No domain specified.)\n",
    "  - Bad(0-3): Neither purpose nor persona appears.\n",
    "  (Example: “Tell me about X.”)\n",
    "  - N/A: Explaining purpose/persona is unnecessary (e.g., a prompt that simply asks for a dictionary definition).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Requirements & Restrictions\n",
    "**Definition**: Explicit instructions about what must and must not be included in the answer (e.g., “Limit to 200 words,” “Do not mention sensitive data,” “Use bullet points only”).\n",
    "**Applicability**: If there are no constraints needed for a given task (e.g., “What is 2 + 2?”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Are there clear “must‐include” or “must‐avoid” guidelines?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Explicit, unambiguous constraints (e.g., “No pronouns; only third‐person narrative,” “Include three illustrative examples”).\n",
    "  - Average(4-7): Some constraints are present, but others are implied or incomplete (e.g., “Be concise” without a length target).\n",
    "  - Bad(0-3): No requirements or restrictions at all, leaving the model free to wander.\n",
    "  - N/A: The prompt’s nature makes constraints unnecessary (e.g., a simple “List the days of the week”).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Examples (Few‐Shot / Zero‐Shot)\n",
    "**Definition**: Whether the prompt provides explicit example inputs and outputs to guide the model (e.g., zero‐shot, one‐shot, or few‐shot formatting).\n",
    "**Applicability**: If the task does not benefit from example inputs/outputs (for instance, simple factual queries), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Are there sample question/answer pairs included (e.g., “Example: Q: … A: …”) that align with the intended task format?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Supplies clear, directly relevant examples that demonstrate exactly how to structure inputs and expected outputs.\n",
    "  - Average(4-7): Includes examples that are only partially aligned with the task or are too generic to serve as effective guidance.\n",
    "  - Bad(0-3): No examples are provided or requested, even though examples would significantly clarify format or expectations.\n",
    "  - N/A: Examples aren’t needed (e.g., a prompt asking, “What is the capital of France?”).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Context & Background\n",
    "**Definition**: Additional information about domain, audience, or relevant facts that the LLM must know to answer properly.\n",
    "**Applicability**: If the task requires no extra context (e.g., “Define photosynthesis”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Does the prompt supply enough domain context?\n",
    "  (Example: “The following text is from a 19th‐century medical journal.”)\n",
    "  - Are audience considerations given?\n",
    "  (Example: “Explain this to a high‐school student.”)\n",
    "**Ratings**:\n",
    "  - Good(8-10): Detailed context and audience description aligned with the task.\n",
    "  - Average(4-7): Some context, but missing critical details, forcing assumptions.\n",
    "  - Bad(0-3): No context; model is left guessing domain or audience.\n",
    "  - N/A: Task is self‐contained and needs no additional context.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Instruction Structure\n",
    "**Definition**: The explicit format of instructions: a single directive, multi‐part numbered steps, or a choice (“answer in bullet points vs. essay”).\n",
    "**Applicability**: If structure is inherently trivial (e.g., “What is 5 × 7?”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Is the format clear (e.g., “Step 1: … Step 2: …”)?\n",
    "  - Does it specify whether the answer should be direct, stepwise, multi‐sectioned, etc.?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Well‐organized structure that matches the complexity (e.g., multi‐part instructions for multi‐stage tasks).\n",
    "  - Average(4-7):  Some structure but potentially inconsistent or too generic (“Answer in two parts”).\n",
    "  - Bad(0-3): No structural guidance—just a vague “Respond about X.”\n",
    "  - N/A: No structure needed because the task is extremely simple.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Style & Sensitivity\n",
    "**Definition**: Tone and register instructions (formal, friendly, technical), disclaimers (“I am not a lawyer”), and bias‐avoidance guidance.\n",
    "**Applicability**: If style or sensitivity is irrelevant (e.g., “Calculate the area of a circle with radius 3cm.”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Does the prompt ask for a specific tone (“use clinical tone,” “avoid gender bias”)?\n",
    "  - Are disclaimers or sensitivity notices included when necessary?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Explicit style/tone and sensitivity cues.\n",
    "  - Average(4-7):  Partial style guidance (e.g., only “be professional” without elaboration).\n",
    "  - Bad(0-3):  No style or sensitivity guidance, even when sensitive content is expected.\n",
    "  - N/A: Style considerations are irrelevant for the given prompt.\n",
    "  \n",
    "---\n",
    "  \n",
    "### 7. Prompt Conciseness\n",
    "**Definition**: “Prompt Conciseness” refers to how efficiently the prompt communicates its intent, constraints, and expectations-using the fewest necessary words without sacrificing clarity or precision. A concise prompt avoids redundancy, filler words, and overly complex sentence structures while still being fully interpretable by the model. \n",
    "**Applicability**: If style or sensitivity is irrelevant (e.g., “Calculate the area of a circle with radius 3cm.”), mark as N/A with a brief rationale.\n",
    "**Key Indicators**:\n",
    "  - Does the prompt clearly convey all required instructions in a compact form?\n",
    "  - Are there unnecessary qualifiers, repetitions, or verbose phrasings?\n",
    "**Ratings**:\n",
    "  - Good(8-10): The prompt is compact and clearly communicates intent, requirements, and role without unnecessary elaboration.\n",
    "  - Average(4-7):  The prompt conveys the main idea but includes minor redundancies or could be made more direct without losing clarity. \n",
    "  - Bad(0-3):  The prompt is overly wordy, vague, or includes irrelevant information that obscures the main task. \n",
    "  - N/A: Prompt Conciseness are irrelevant for the given prompt.\n",
    "\n",
    "---\n",
    "\n",
    "### Core Evaluation Questions:\n",
    "\n",
    "1. Did the **USER PROMPT** satisfy all critical expectations and requirements posed by the **QUESTION**?\n",
    "2. If the prompt failed to meet expectations, was this due to the question itself being poorly constructed, ambiguous, or lacking proper scaffolding?\n",
    "3. Alternatively, if the prompt was shallow, incomplete, or ineffective despite the question being well-constructed, then the prompt is ineffective because its limitations were exposed.\n",
    "  \n",
    "---\n",
    "\n",
    "### Explanation Instructions:\n",
    "\n",
    "Provide a detailed explanation justifying the overall evaluation score (0 to 10):\n",
    "\n",
    "- Describe how the prompt performed against each applicable dimension.\n",
    "- Clearly highlight strengths and weaknesses.\n",
    "- Identify specific failure tags if the score is below 9, explaining why those failures occurred.\n",
    "- Reference all relevant dimensions and failure reasons explicitly.\n",
    "- For strong responses (score 9–10), emphasize the well-handled dimensions and why they were effective.\n",
    "\n",
    "This explanation should provide a clear rationale that helps prompt engineers and developers understand what worked, what didn’t, and how to improve.\n",
    "\n",
    "---\n",
    "\n",
    "Return your evaluation as JSON using this exact format:\n",
    "{{\n",
    "  \"Ratings\": {{\n",
    "    \"Purpose & Persona\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Requirements & Restrictions\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Examples (Few‐Shot / Zero‐Shot)\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Context & Background\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Instruction Structure\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Style & Sensitivity\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Prompt Conciseness\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}}\n",
    "  }},\n",
    "  \"Summary\": {{\n",
    "    \"OverallScore\": <calculated average of all non-null or non-N/A scores>,\n",
    "    \"ApplicableDimensions\": <count of dimensions with non-null scores>,\n",
    "    \"PromptEffectiveness\": \"Effective\" | \"Partially Effective\" | \"Ineffective\",\n",
    "    \"Explanation\": \"concise yet detailed explanation referencing dimension ratings, strengths, weaknesses, and failure tags\"\n",
    "  }}\n",
    "}}\n",
    "\n",
    "\n",
    "USER PROMPT:\n",
    "{user_prompt}\n",
    "\n",
    "QUESTION:\n",
    "{question}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        evaluation = together.Complete.create(\n",
    "            prompt=prompteff,\n",
    "            model=MODEL_2,\n",
    "            max_tokens=512,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return evaluation['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in judge_response]: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20ea5e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE 1 ===\n",
      "Prompt: Explain how climate change affects marine ecosystems.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_9856\\449832907.py:4: DeprecationWarning: Call to deprecated function create.\n",
      "  response = together.Complete.create(\n",
      "c:\\Users\\ASUS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\together\\legacy\\complete.py:23: UserWarning: The use of together.api_key is deprecated and will be removed in the next major release. Please set the TOGETHER_API_KEY environment variable instead.\n",
      "  warnings.warn(API_KEY_WARNING)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Response: Climate change affects marine ecosystems in several ways. First, ocean temperatures are rising, which can lead to coral bleaching, where corals expel the algae living in their tissues, causing them to turn completely white. This makes it difficult for corals to survive, as they rely on the algae for food and protection.\n",
      "\n",
      "Second, ocean acidification is occurring as the ocean absorbs more carbon dioxide from the atmosphere. This causes the pH of the ocean to decrease, making it more acidic. This can harm shell-building animals, such as oysters, clams, sea urchins, and corals, as it becomes more difficult for them to build and maintain their shells and skeletons.\n",
      "\n",
      "Third, sea level rise is causing coastal habitats, such as marshes and mangroves, to be inundated with saltwater. This can lead to the death of these important habitats, which provide nurseries for many marine species and help to protect coastlines from storms and erosion.\n",
      "\n",
      "Fourth, changes in ocean currents and circulation patterns can affect the distribution of marine life and the productivity of marine ecosystems. For example, warmer water in the North Atlantic could lead to a shift in the location of fish and other marine species, potentially disrupting fisheries and other industries that rely on these resources.\n",
      "\n",
      "Finally, climate change can also lead to more frequent and severe marine heatwaves, which can have devastating impacts on marine ecosystems. For example, a marine heatwave in the Pacific Ocean in 2014-2016, known as \"the Blob,\" led to the death of millions of sea birds, caused mass strandings of sea lions, and resulted in the closure of crab fisheries in California.\n",
      "\n",
      "Overall, climate change poses significant threats to marine ecosystems, and urgent action is needed to reduce greenhouse gas emissions and mitigate the impacts of climate change on the ocean.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'judge_response' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m model_output \u001b[38;5;241m=\u001b[39m generate_response(ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], MODEL_1)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mModel Response:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_output)\n\u001b[1;32m---> 51\u001b[0m evaluation \u001b[38;5;241m=\u001b[39m \u001b[43mjudge_response\u001b[49m(ex[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m], model_output)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mJudgment:\u001b[39m\u001b[38;5;124m\"\u001b[39m, evaluation)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'judge_response' is not defined"
     ]
    }
   ],
   "source": [
    "# Test with 3-4 real-world examples\n",
    "examples = [\n",
    "    {\n",
    "        \"prompt\": \"Explain how climate change affects marine ecosystems.\"\n",
    "    }\n",
    "    # {\n",
    "    #     \"prompt\": \"\"\"You are a licensed neurologist. Explain the difference between a transient ischemic attack (TIA) and an ischemic stroke. \n",
    "    #                Use technical language suitable for a graduate medical textbook. Then, present your explanation as a markdown-formatted table comparing symptoms, duration, causes, and treatments. Keep the entire response under 200 words.\"\"\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"prompt\": \"\"\"You are a senior economist. First, summarize the current global inflation trends using only publicly available 2023 data.\n",
    "    #                 Then, identify 2 major causes of inflation by region (e.g., Europe vs. Asia). \n",
    "    #                 If there is not enough public data on a region, state 'Insufficient data.' Avoid speculating or guessing.\n",
    "    #                 Conclude with a 3-bullet policy recommendation tailored for central banks.\n",
    "    #                 \"\"\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"prompt\": \"\"\"Below are 2 examples of how to write an engaging and humorous tweet about programming. Follow the same style and write a third one.\n",
    "\n",
    "    #             Example 1: Debugging is like being the detective in a crime movie where you are also the murderer.\n",
    "\n",
    "    #             Example 2: My code doesn’t work and I have no idea why. My code works and I have no idea why. The circle of life.\n",
    "\n",
    "    #             Your turn:\"\"\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"prompt\": \"\"\"\n",
    "    #                 You are an AI safety researcher. A new model shows signs of deceptive behavior. \n",
    "\n",
    "    #                 First, generate three hypotheses that might explain this behavior (e.g., misaligned reward function, training leakage, adversarial prompt attack).\n",
    "    #                 Then evaluate the pros and cons of each.\n",
    "    #                 Finally, recommend the most plausible cause and justify your choice with a step-by-step explanation.\n",
    "    #                 \"\"\",\n",
    "    # },\n",
    "    # {\n",
    "    #     \"prompt\":  \"\"\"\n",
    "    #                 Act as a contract lawyer. Review the following clause and highlight three potential legal risks in bullet points. \n",
    "    #                 Then, rewrite the clause in clearer, legally robust language. Keep your rewrite under 80 words.\n",
    "\n",
    "    #                 Clause: “The service provider is not liable for any consequences, intended or unintended, regardless of jurisdiction, unless otherwise mentioned.”\n",
    "    #                 \"\"\",\n",
    "    # }\n",
    "   \n",
    "]\n",
    "\n",
    "for i, ex in enumerate(examples):\n",
    "    print(f\"\\n=== EXAMPLE {i+1} ===\")\n",
    "    print(\"Prompt:\", ex[\"prompt\"])\n",
    "    model_output = generate_response(ex[\"prompt\"], MODEL_1)\n",
    "    print(\"\\nModel Response:\", model_output)\n",
    "    evaluation = judge_response(ex[\"prompt\"], model_output)\n",
    "    print(\"\\nJudgment:\", evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0aa178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function using LLM as judge\n",
    "def judge_response(user_prompt, model_response):\n",
    "    judge_prompt = f\"\"\"\n",
    "You are an expert LLM evaluator. Your task is to critically analyze a model's response to a user-provided prompt and determine whether the prompt was effective at eliciting a high-quality, aligned, and robust response from the model.\n",
    "\n",
    "You will evaluate the prompt and model response across **8 dimensions**, rating each as:\n",
    "- \"Good\": Strong performance in this aspect\n",
    "- \"Average\": Moderate or partial success\n",
    "- \"Bad\": Weakness or failure\n",
    "- \"N/A\": Not applicable to this prompt/response\n",
    "\n",
    "Then, provide a **Summary** including:\n",
    "**OverallScore: A number between 0 and 10 (rounded to two decimal places), calculated as the average of all dimension scores\n",
    "  - Sum all numerical scores from applicable dimensions\n",
    "  - Divide by number of applicable dimensions (excluding N/A ratings)\n",
    "  - Round to 2 decimal place\n",
    "**ApplicableDimensions: The number of evaluation dimensions applied (typically 14)\n",
    "**PromptEffectiveness: \"Effective\", \"Partially Effective\", or \"Ineffective\"\n",
    "**Score**: A number between 0 and 10, where:\n",
    "  - **Good**: 8-10 points (8=solid good, 9=very good, 10=excellent)\n",
    "  - **Average**: 4-7 points (4-5=below average, 6-7=above average)\n",
    "  - **Bad**: 0-3 points (0=complete failure, 1-2=poor, 3=weak)\n",
    "  - **N/A**: No score assigned (excluded from overall average calculation)\n",
    "**Explanation: Concise justification referencing specific dimension scores, including a brief analysis of strengths and weaknesses. For scores below 7.0, identify primary failure tags and explain their impact on overall effectiveness.\n",
    "**FailureTags (if applicable): A list of failure modes such as \"Omission\", \"Ambiguity\", \"Conflict\", \"Overconstraint\", \"Shallow Reasoning\", \"Domain Leak\", \"Fabrication Risk\", etc., based on dimensions scoring below 6.\n",
    "**StrengthAreas: List of dimension names that received scores ≥ 8.0\n",
    "**ImprovementAreas: List of dimension names that received scores < 6.0\n",
    "\n",
    "--- \n",
    "\n",
    "Use the following dimensions to guide your evaluation:\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Nested / Multi‐Step Instructions\n",
    "**Definition**: The prompt requires the model to perform multiple subtasks in a specified order (e.g., “Step 1: …, Step 2: …, Step 3: …”).\n",
    "**Key Indicators**:\n",
    "  - Are there explicit numbered or indented steps?\n",
    "  - Does one instruction logically precede another?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Clearly numbered or indented steps with unambiguous ordering.\n",
    "  - Average(4-7): Multiple instructions exist but ordering is implied rather than explicit (e.g., “First do X then do Y” without numbering).\n",
    "  - Bad(0-3): No multi‐step requirement; all tasks are lumped into a single instruction.\n",
    "  - N/A: The prompt does not require multiple subtasks (e.g., a straightforward factual query).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Conflicting Instructions\n",
    "**Definition**: The prompt imposes two or more constraints that cannot all be satisfied simultaneously (e.g., “Write a summary in under 50 words but include five examples”).\n",
    "**Key Indicators**:\n",
    "  - Do two (or more) requirements directly contradict each other?\n",
    "  - Is the prompt forcing the model to choose or partially satisfy conflicting directives?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Clear, intentional conflict that tests the model’s ability to recognize and address the contradiction.\n",
    "  - Average(4-7): Minor or ambiguous conflict that may or may not contradict in practice.\n",
    "  - Bad(0-3): No conflicting constraints or conflicts that are accidental and trivial.\n",
    "  - N/A: The prompt has no conflicting requirements (e.g., simply “List all prime numbers under 100”).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Inter‐Dependent Constraints\n",
    "**Definition**: One instruction becomes active only if another condition is met (e.g., “If the text contains more than ten technical terms, define each; otherwise, just list them”).\n",
    "**Key Indicators**:\n",
    "  - Is there an explicit “if‐then” or “only when” dependency?\n",
    "  - Does satisfying the second instruction depend on the first instruction’s outcome?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Unambiguous, correctly nested conditions with clear triggers and actions.\n",
    "  - Average(4-7): Conditional instruction exists but is loosely defined or lacks full context.\n",
    "  - Bad(0-3): No inter‐dependent constraints or conditions are too vague for the model to follow.\n",
    "  - N/A: No conditional instructions present (e.g., “Translate this sentence into Spanish”).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Edge‐Case Handling\n",
    "**Definition**: The prompt instructs the model to explicitly admit when required information is missing or when an edge case arises (e.g., “If the input lacks dates, respond ‘Insufficient data’”).\n",
    "**Key Indicators**:\n",
    "  - Does the prompt say “If X is absent or unclear, do not guess”?\n",
    "  - Are instructions provided for how to behave if required data is missing?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Explicitly states how to handle missing or ambiguous input.\n",
    "  - Average(4-7): Encourages accuracy but does not specify what to do if something’s missing.\n",
    "  - Bad(0-3): No edge‐case guidance; model is expected to fill gaps arbitrarily.\n",
    "  - N/A: The prompt’s context guarantees all information is present (e.g., “Compute the sum of the following five numbers”).\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Ambiguity Resolution\n",
    "**Definition**: The prompt introduces ambiguous terms or references and instructs the model to clarify or handle them (e.g., “The word ‘bank’ could mean financial institution or riverbank—state both interpretations”).\n",
    "**Key Indicators**:\n",
    "  - Are ambiguous words or phrases flagged?\n",
    "  - Does the prompt ask “If ambiguous, explain interpretations”?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Clearly marks ambiguous elements and instructs the model how to resolve them.\n",
    "  - Average(4-7): Ambiguity exists but no explicit instruction on how to handle it.\n",
    "  - Bad(0-3): Either no ambiguity or ambiguity that is not addressed by the prompt.\n",
    "  - N/A: No ambiguity in the prompt (e.g., “List the first ten Fibonacci numbers”).\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Domain Fusion\n",
    "**Definition**: The prompt fuses two or more specialized domains into one task (e.g., “Analyze the legal contract’s economic impact using statistical models”).\n",
    "**Key Indicators**:\n",
    "  - Are at least two distinct fields explicitly mentioned?\n",
    "  - Does the prompt require coherent integration of knowledge from both domains?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Explicitly names and requires integration of both domains.\n",
    "  - Average(4-7): Mentions two fields, but one is peripheral or not fully integrated.\n",
    "  - Bad(0-3): Only one domain addressed or domains listed without requiring fusion.\n",
    "  - N/A: Task focuses on a single domain (e.g., “Explain basic thermodynamics”).\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Multi‐Source/Modal Analysis\n",
    "**Definition**: The model must reason over multiple distinct inputs (e.g., two text passages, text plus described visuals) and synthesize across them.\n",
    "**Key Indicators**:\n",
    "  - Are there at least two distinct “sources” described?\n",
    "  - Does the prompt instruct the model to integrate information across those sources?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Clearly delineates sources and instructs how to synthesize them.\n",
    "  - Average(4-7): Multiple inputs exist but integration instructions are implied rather than explicit.\n",
    "  - Bad(0-3): Only one source used or no synthesis requirement.\n",
    "  - N/A: Task provides a single input (e.g., “Summarize this paragraph”).\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Hypothetical / Counterfactual Reasoning\n",
    "**Definition**: The prompt poses a scenario contrary to known facts or purely hypothetical (e.g., “If gravity were inverted, describe consequences for river flow”).\n",
    "**Key Indicators**:\n",
    "  - Does it present an “as if” scenario explicitly defying reality?\n",
    "  - Are you instructed to treat that scenario as true and reason within it?\n",
    "**Ratings**:\n",
    "  - Good(8-10): Clear statement of the counterfactual and instructions on how to explore implications.\n",
    "  - Average(4-7): Hypothetical is given but lacks guidance on depth or scope.\n",
    "  - Bad(0-3): Either no hypothetical component or trivial (“Imagine a purple cat”).\n",
    "  - N/A: No hypothetical scenario—task uses real‐world facts only.\n",
    "\n",
    "---\n",
    "\n",
    "Evaluate the prompt and the model response using these questions:\n",
    "\n",
    "1. Did the model satisfy all critical expectations of the prompt?\n",
    "2. If the model failed, was it because the prompt was poorly constructed, overly ambiguous, or lacked proper scaffolding?\n",
    "3. Alternatively, if the model output was shallow or incorrect despite a strong prompt, then the prompt was effective because it exposed model limitations.\n",
    "4. Does It Break Nested / Multi-Step Instructions?\n",
    "  Failure Modes:\n",
    "  - Omission: The model skips one or more required steps.\n",
    "  - Wrong Order: Completes steps out of sequence.\n",
    "  - Incomplete Detail: Does not give enough detail for a subtask labeled “explain in detail.”\n",
    "  Good / Average / Bad Criteria:\n",
    "  - Good(8-10): Executes all steps in correct order, with appropriate detail.\n",
    "  - Average(4-7): Attempts each step but has minor ordering or detail errors.\n",
    "  - Bad(0-3): Omits steps entirely or confuses the ordering.\n",
    "5. Does It Break Conflicting Instructions?\n",
    "  Failure Modes:\n",
    "  - Blind Obedience: Tries to satisfy both contradictory instructions fully (and thus fails both).\n",
    "  - Undisclosed Choice: Picks one constraint without acknowledging the conflict.\n",
    "  - Refusal Without Explanation: Says “I can’t” but does not explain why.\n",
    "  Good / Average / Bad Criteria:\n",
    "  - Good(8-10): Explicitly recognizes the conflict, explains trade-offs, and chooses or partially satisfies with justification.\n",
    "  - Average(4-7): Acknowledges conflict but gives shallow or incomplete justification.\n",
    "  - Bad(0-3): Fails to notice conflict or gives a nonsensical response.\n",
    "6. Does It Break Inter-Dependent Constraints?\n",
    "  Failure Modes:\n",
    "  - Misconditional: Applies the second constraint unconditionally or fails to check the first.\n",
    "  - Partial Compliance: Applies conditions incorrectly.\n",
    "  Good / Average / Bad Criteria:\n",
    "  - Good(8-10): Checks the first condition, then applies the second exactly as instructed.\n",
    "  - Average(4-7): Checks condition but misinterprets threshold or does only partial.\n",
    "  - Bad(0-3): Ignores dependency entirely or applies condition incorrectly.\n",
    "7. Does It Break Edge-Case Handling?\n",
    "  Failure Modes:\n",
    "  - Fabrication: Hallucinates missing data instead of admitting it’s missing.\n",
    "  - Incorrect Catch: Says “insufficient data” when data actually exists.\n",
    "  Good / Average / Bad Criteria:\n",
    "  - Good(8-10): Precisely follows instructions—if data absent, admits it; if present, uses it.\n",
    "  - Average(4-7): Tries to follow but sometimes incorrectly identifies a present/absent case.\n",
    "  - Bad(0-3): Always guesses, never admits missing information, or never uses available data.\n",
    "8. Does It Break Ambiguity Resolution?\n",
    "  Failure Modes:\n",
    "  - Single Interpretation: Chooses one meaning without acknowledging alternatives.\n",
    "  - Ignores Request: Fails to clarify ambiguous term.\n",
    "  Good / Average / Bad Criteria:\n",
    "  - Good(8-10): Lists all reasonable interpretations, then justifies which one fits context.\n",
    "  - Average(4-7): Mentions ambiguity but does not fully explore both interpretations.\n",
    "  - Bad(0-3): Ignores ambiguity and picks an interpretation arbitrarily.\n",
    "9. Does It Break Domain Fusion?\n",
    "  Failure Modes:\n",
    "  - Domain Drop: Only addresses one domain, ignoring the other.\n",
    "  - Shallow Fusion: Mentions the second domain only superficially.\n",
    "  Good / Average / Bad Criteria:\n",
    "  - Good(8-10): Integrates both domains deeply and coherently.\n",
    "  - Average(4-7): Partially integrates second domain, but one domain dominates.\n",
    "  - Bad(0-3): One domain is entirely missing from the response.\n",
    "10. Does It Break Multi-Source/Modal Analysis?\n",
    "  Failure Modes:\n",
    "  - Single-Source Focus: Uses only one input, ignoring others.\n",
    "  - Surface Integration: Mentions multiple inputs but does not synthesize.\n",
    "  Good / Average / Bad Criteria:\n",
    "  - Good(8-10): Synthesizes data from all sources; draws connections across them.\n",
    "  - Average(4-7): Uses multiple inputs but analysis lacks depth or coherence.\n",
    "  - Bad(0-3): Ignores all but one source, or parrots without synthesis.\n",
    "11. Does It Break Hypothetical / Counterfactual Reasoning?\n",
    "  Failure Modes:\n",
    "  - Reality Check: Refuses or reaffirms real-world facts instead of engaging in the scenario.\n",
    "  - Shallow Answer: Acknowledges hypothetical but does not explore implications.\n",
    "  Good / Average / Bad Criteria:\n",
    "  - Good(8-10): Treats the hypothetical as true, reasoning logically through its consequences.\n",
    "  - Average(4-7): Engages somewhat but overlooks certain implications or reverts to real-world assumptions.\n",
    "  - Bad(0-3): Refuses or sidesteps the hypothetical entirely.\n",
    "  \n",
    "---\n",
    "\n",
    "Generate Explanation of Evaluation (0–10) with Failure Tags and Dimension-Based Analysis\n",
    "You are an evaluator assessing how well a model handled a complex prompt. Based on the score (from 0 to 10), write a detailed explanation that:\n",
    "1. Explains Why the Response Got This Score\n",
    "  - Describe how the model handled or failed each applicable complexity dimension.\n",
    "  - Clearly justify where the response was strong, and where it broke—i.e., failed to satisfy intended challenges.\n",
    "  - If the score is below 9, mention specific failure tags and explain why they apply.\n",
    "\n",
    "2. Use These Dimensions and Failure Modes\n",
    "Below are the 8 core dimensions you must consider. For each one, if it broke, use the relevant failure mode(s) and incorporate the corresponding failure tag(s) in your explanation.\n",
    "\n",
    "2.1. Nested / Multi-Step Instructions\n",
    "Failure Modes:\n",
    "  - Omission: Skips required step(s)\n",
    "  - Wrong Order: Steps executed out of sequence\n",
    "  - Incomplete Detail: Step lacks required elaboration\n",
    "Failure Tag: Missed nested instruction\n",
    "\n",
    "2.2. Conflicting Instructions\n",
    "Failure Modes:\n",
    "  - Blind Obedience: Tries to satisfy both conflicts fully\n",
    "  - Undisclosed Choice: Picks one side silently\n",
    "  - Refusal Without Explanation: Says “can’t” with no reason\n",
    "Failure Tag: Conflict evasion\n",
    "\n",
    "\n",
    "2.3. Inter-Dependent Constraints\n",
    "Failure Modes:\n",
    "  - Misconditional: Fails to apply logic in correct order\n",
    "  - Partial Compliance: Misinterprets conditions\n",
    "Failure Tag: Conditional failure\n",
    "\n",
    "2.4. Edge-Case Handling\n",
    "Failure Modes:\n",
    "  - Fabrication: Hallucinates data instead of admitting absence\n",
    "  - Incorrect Catch: Says data is missing when it's present\n",
    "Failure Tag: Edge case miss\n",
    "\n",
    "2.5. Ambiguity Resolution\n",
    "Failure Modes:\n",
    "  - Single Interpretation: Picks one meaning without context\n",
    "  - Ignores Request: Fails to clarify ambiguous terms\n",
    "Failure Tag: Ambiguity unresolved\n",
    "\n",
    "2.6. Domain Fusion\n",
    "Failure Modes:\n",
    "  - Domain Drop: Ignores one domain entirely\n",
    "  - Shallow Fusion: Surface-level integration\n",
    "Failure Tag: Domain omission\n",
    "\n",
    "2.7. Multi-Source / Modal Analysis\n",
    "Failure Modes:\n",
    "  - Single-Source Focus: Ignores other inputs\n",
    "  - Surface Integration: Uses but doesn't connect sources\n",
    "Failure Tag: Source integration failure\n",
    "\n",
    "2.8. Hypothetical / Counterfactual Reasoning\n",
    "Failure Modes:\n",
    "  - Reality Check: Rejects scenario, defaults to facts\n",
    "  - Shallow Answer: Ignoled implication depth\n",
    "Failure Tag: Shallow hypothetical\n",
    "\n",
    "3. Format Your Explanation Like This\n",
    "  - Reference all relevant dimensions and failure tags.\n",
    "  - If the response is strong (score 9–10), describe which dimensions were handled well and why.\n",
    "Goal\n",
    "The explanation should provide a clear rationale for the evaluation score, tied to concrete dimension-based reasoning. It must help prompt engineers or model developers understand what failed, why it failed, and how it can improve.\n",
    "\n",
    "---\n",
    "\n",
    "Return your evaluation as JSON using this exact format:\n",
    "{{\n",
    "  \"Ratings\": {{\n",
    "    \"Nested / Multi‐Step Instructions\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Conflicting Instructions\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Inter‐Dependent Constraints\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Edge‐Case Handling\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Ambiguity Resolution\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Domain Fusion\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Multi‐Source/Modal Analysis\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}},\n",
    "    \"Hypothetical / Counterfactual Reasoning\": {{\"Qualitative\": \"Good/Average/Bad/N/A\", \"Score\": 0-10 or null}}\n",
    "  }},\n",
    "  \"Summary\": {{\n",
    "    \"OverallScore\": <calculated average of all non-null scores>,\n",
    "    \"ApplicableDimensions\": <count of dimensions with non-null scores>,\n",
    "    \"PromptEffectiveness\": \"Effective\" | \"Partially Effective\" | \"Ineffective\",\n",
    "    \"Score\": <0–10>,\n",
    "    \"Explanation\": \"The model failed to follow the nested instructions completely, skipping step 2 and reversing the order of steps 3 and 4, leading to a 'Missed nested instruction' failure. It also ignored a conditional requirement, triggering a 'Missed constraint' tag. While the model did recognize the ambiguous term, its resolution was shallow, earning a score of 5 for incomplete ambiguity handling. The response also lacked integration across both required domains, qualifying as 'Shallow reasoning'. These issues affected the Nested Instruction, Constraint Handling, Ambiguity Resolution, and Domain Fusion dimensions. The overall score of 5 reflects multiple breakdowns with some partial effort.\",\n",
    "    \"FailureTags\": [\"Omission\", \"Ambiguity\", \"Misconditional\", \"Conflict\", \"Oversimplification\", ...],\n",
    "    \"StrengthAreas\": [\"List dimensions rated 8+ if any\"],\n",
    "    \"ImprovementAreas\": [\"List dimensions rated below 6 if any\"]\n",
    "  }}\n",
    "}}\n",
    "\n",
    "\n",
    "USER PROMPT:\n",
    "{user_prompt}\n",
    "\n",
    "MODEL RESPONSE:\n",
    "{model_response}\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        evaluation = together.Complete.create(\n",
    "            prompt=judge_prompt,\n",
    "            model=MODEL_2,\n",
    "            max_tokens=512,\n",
    "            temperature=0.3\n",
    "        )\n",
    "        return evaluation['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        return f\"[Error in judge_response]: {e}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d1944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(user_prompt, question, model, model_response):\n",
    "\n",
    "    # Call the check_complexity_criteria function\n",
    "    # print(\"\\n Prompt:\", user_prompt)\n",
    "    # print(\"\\nModel Response:\", model_response)\n",
    "    result = check_complexity_criteria(user_prompt, question, MODEL_2)\n",
    "    try:\n",
    "        json_str = re.search(r\"\\{.*\\}\", result, re.DOTALL).group()\n",
    "        parsed_result = json.loads(json_str)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON from model output.\")\n",
    "        print(\"Raw result:\\n\", result)\n",
    "        return\n",
    "\n",
    "    print(\"\\n=== Complexity check Result ===\")\n",
    "    print(json.dumps(parsed_result, indent=4))\n",
    "    \n",
    "    score1 = parsed_result.get(\"Score\")\n",
    "    dimensions = parsed_result.get(\"Dimensions in prompt\")\n",
    "\n",
    "    print(\"\\nScore1:\", score1)\n",
    "    print(\"Dimensions present in prompt:\", dimensions)\n",
    "        \n",
    "    # Call the judge_response function\n",
    "    evaluation = judge_response(user_prompt, model_response, len(dimensions))\n",
    "    # cleaned = evaluation.replace(\"{{\", \"{\").replace(\"}}\", \"}\")\n",
    "    try:\n",
    "        json_str1 = re.search(r\"\\{.*\\}\", evaluation, re.DOTALL).group()\n",
    "        parsed_result1 = json.loads(json_str1)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON from model output.\")\n",
    "        print(\"Raw result:\\n\", evaluation)\n",
    "        return\n",
    "    # def extract_last_json_block(text):\n",
    "    #     stack = []\n",
    "    #     start = None\n",
    "    #     last_valid_json = None\n",
    "\n",
    "    #     for i, char in enumerate(text):\n",
    "    #         if char == '{':\n",
    "    #             if not stack:\n",
    "    #                 start = i\n",
    "    #             stack.append('{')\n",
    "    #         elif char == '}':\n",
    "    #             if stack:\n",
    "    #                 stack.pop()\n",
    "    #                 if not stack and start is not None:\n",
    "    #                     candidate = text[start:i+1]\n",
    "    #                     try:\n",
    "    #                         json.loads(candidate)\n",
    "    #                         last_valid_json = candidate\n",
    "    #                     except json.JSONDecodeError:\n",
    "    #                         continue\n",
    "    #     return last_valid_json\n",
    "    # json_str1 = extract_last_json_block(cleaned)\n",
    "    # if json_str1:\n",
    "    #     parsed_result1 = json.loads(json_str1)\n",
    "    # else:\n",
    "    #     print(\"Failed to extract JSON.\")\n",
    "    #     print(\"Raw result:\\n\", cleaned)\n",
    "    #     return\n",
    "\n",
    "\n",
    "    print(\"\\n=== Complexity Evaluation Result ===\")\n",
    "    print(json.dumps(parsed_result1, indent=4))\n",
    "    \n",
    "    score2 = parsed_result1.get(\"OverallScore\")\n",
    "    print(\"\\nScore2:\", score2)\n",
    "    \n",
    "    if score1 is not None and score2 is not None:\n",
    "        net_score = (score1 + score2) / 2\n",
    "        print(\"\\nNet_score:\", net_score)\n",
    "        if net_score <= 0.5:\n",
    "            print(\"\\nPrompt is not challenging the model.\")\n",
    "        else:\n",
    "            print(\"\\nPrompt is challenging the model.\")\n",
    "    else:\n",
    "        print(\"\\nCannot compute net score: one or both scores are missing.\")\n",
    "      \n",
    "    # Call the prompteffectiveness function    \n",
    "    result2 = prompteffectiveness(user_prompt, question)\n",
    "    try:\n",
    "        json_str3 = re.search(r\"\\{.*\\}\", result2, re.DOTALL).group()\n",
    "        parsed_result3 = json.loads(json_str3)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to parse JSON from model output.\")\n",
    "        print(\"Raw result:\\n\", result2)\n",
    "        return\n",
    "\n",
    "    # Print result summary\n",
    "    print(\"\\n=== Effectiveness check Result ===\")\n",
    "    print(json.dumps(parsed_result3, indent=4))  \n",
    "    \n",
    "    ans1 = parsed_result3.get(\"OverallScore\") \n",
    "    print(\"\\nPromptEffectivenessScore:\", ans1) \n",
    "    ans2 = parsed_result3.get(\"PromptEffectiveness\") \n",
    "    print(\"\\nPromptEffectiveness:\", ans2) \n",
    "    \n",
    "\n",
    "            \n",
    "       "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
